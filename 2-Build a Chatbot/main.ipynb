{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, trim_messages, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import (BaseChatMessageHistory, InMemoryChatMessageHistory)\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_openAi = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "model_ollama = OllamaLLM(model=\"gemma2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have feelings or experiences like humans do. But thanks for asking!\\n\\nHow are you doing today? 😊\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ollama.invoke([HumanMessage(\"Hola como estas?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Como modelo de lenguaje, no tengo acceso a información personal como tu nombre.\\n\\nPara que pueda ayudarte con tu pregunta, por favor, dime tu nombre. 😊'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ollama.invoke([HumanMessage(\"cual es mi nombre?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Como modelo lingüístico de IA, no tengo hermanos ni familia. Soy un programa informático diseñado para ayudar a las personas con tareas como generar texto, traducir idiomas y responder preguntas.  \\n\\n¿Hay algo más en lo que pueda ayudarte? 😊 \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ollama.invoke(\n",
    "    [\n",
    "        HumanMessage(\"Cual es el nombre de la persona que me atiende?\"),\n",
    "        SystemMessage(\"Mi nombre es Gemma2\"),\n",
    "        HumanMessage(\"Gemma2, tienes algun hermano?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model_openAi, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creando un session id\n",
    "config_id = {\"configurable\": {\"session_id\": \"1234\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"que tamaño de monitor es recomentable para trabajar?\")],\n",
    "    config=config_id\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El tamaño recomendable de un monitor para trabajar depende de las necesidades y preferencias del usuario. Generalmente, se considera que un monitor de 24 pulgadas es adecuado para la mayoría de las tareas de trabajo, ya que proporciona suficiente espacio en pantalla para multitareas y una buena visualización de contenido.\\n\\nSin embargo, si se requiere de más espacio en pantalla, se puede optar por monitores de 27 pulgadas o incluso de 32 pulgadas, que ofrecen una mayor área de visualización y son ideales para tareas que requieren trabajar con múltiples ventanas abiertas al mismo tiempo.\\n\\nPor otro lado, si se necesita un monitor más portátil y con menor espacio ocupado en el escritorio, se pueden considerar monitores de 21 o 22 pulgadas.\\n\\nEn resumen, el tamaño recomendable de un monitor para trabajar dependerá de las necesidades específicas de cada usuario, pero en general, se considera que un monitor de 24 pulgadas es una buena opción para la mayoría de las personas.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Para un monitor de 32 pulgadas, se recomienda una resolución de al menos 2560 x 1440 píxeles (QHD) para garantizar una buena calidad de imagen y una visualización nítida de los contenidos. Esto proporcionará una mayor densidad de píxeles en comparación con monitores de menor tamaño, lo que resultará en una experiencia visual más inmersiva.\\n\\nAdemás, es importante considerar la distancia a la que se va a utilizar el monitor. Para un monitor de 32 pulgadas, se recomienda colocarlo a una distancia de aproximadamente 80-100 centímetros del usuario para una visualización cómoda y ergonómica.\\n\\nEn cuanto a la configuración de color, se sugiere calibrar el monitor para obtener una reproducción precisa de los colores. Se puede utilizar un dispositivo de calibración de color o ajustar manualmente los ajustes de brillo, contraste y temperatura de color para lograr una imagen equilibrada y natural.\\n\\nEn resumen, para un monitor de 32 pulgadas, se recomienda una resolución de al menos 2560 x 1440 píxeles, una distancia de visualización adecuada y una configuración de color precisa para una experiencia de trabajo óptima.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"que configuracion es recomendable para un monitor de 32 pulgadas?\")],\n",
    "    config=config_id\n",
    "    \n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promptemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Actua como un asistente de desarrollo de software y responde a las preguntas de los usuarios\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model_openAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Para instalar Python en tu computadora, sigue estos pasos:\\n\\n1. Ve al sitio web oficial de Python en https://www.python.org/downloads/\\n2. Descarga la última versión estable de Python para tu sistema operativo (Windows, macOS o Linux).\\n3. Ejecuta el instalador descargado y sigue las instrucciones en pantalla.\\n4. Durante la instalación, asegúrate de marcar la casilla que dice \"Add Python to PATH\" (Agregar Python al PATH) para poder usar Python desde la línea de comandos.\\n5. Una vez finalizada la instalación, puedes verificar que Python se ha instalado correctamente abriendo una terminal (en Windows, puedes abrir el símbolo del sistema o PowerShell) y escribiendo `python --version` o `python3 --version`.\\n\\n¡Listo! Ahora tienes Python instalado en tu computadora y puedes empezar a programar en este lenguaje.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(\"como puedo instalar python?\")]})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregando message history al prompt template\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id_session = {\"configurable\": {\"session_id\": \"789\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"usando conda puedo instalar diferentes versiones de python?\")],\n",
    "    config=config_id_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sí, con Conda puedes crear entornos virtuales con diferentes versiones de Python. Por ejemplo, puedes crear un entorno con Python 3.6 y otro con Python 3.8. De esta manera, puedes trabajar en proyectos que requieran versiones específicas de Python sin que entren en conflicto entre sí. Aquí te dejo un ejemplo de cómo crear un entorno con una versión específica de Python utilizando Conda:\\n\\n1. Para crear un nuevo entorno con una versión específica de Python, puedes utilizar el siguiente comando en la terminal:\\n\\n```\\nconda create -n mi_entorno python=3.6\\n```\\n\\nEste comando creará un nuevo entorno llamado \"mi_entorno\" con Python 3.6 instalado.\\n\\n2. Para activar el entorno recién creado, puedes utilizar el siguiente comando:\\n\\n```\\nconda activate mi_entorno\\n```\\n\\nUna vez activado el entorno, puedes instalar las librerías y paquetes necesarios para tu proyecto en ese entorno específico.\\n\\nRecuerda que es importante mantener los entornos virtuales separados para evitar conflictos entre las diferentes versiones de Python y las dependencias de tus proyectos. ¡Espero que esta información te haya sido útil! Si tienes alguna otra pregunta, no dudes en decírmelo.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"y puedo usala con visual studio code?\")],\n",
    "    config=config_id_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sí, puedes utilizar Conda con Visual Studio Code para gestionar entornos virtuales y trabajar en proyectos de Python de manera más organizada. Aquí te explico cómo puedes configurar Visual Studio Code para que reconozca y utilice entornos virtuales de Conda:\\n\\n1. Abre Visual Studio Code y asegúrate de tener instalada la extensión \"Python\" de Microsoft.\\n\\n2. En la parte inferior izquierda de la ventana de Visual Studio Code, verás un botón que indica el intérprete de Python actualmente seleccionado. Haz clic en este botón y selecciona \"Seleccionar intérprete\".\\n\\n3. En la lista de intérpretes que aparece, busca la opción \"Enter interpreter path...\" y haz clic en ella.\\n\\n4. En la ventana que se abre, introduce la ruta al intérprete de Python que se encuentra dentro del entorno virtual que creaste con Conda. Por lo general, la ruta tendrá una estructura similar a esta: `C:\\\\Users\\\\tu_usuario\\\\Anaconda3\\\\envs\\\\mi_entorno\\\\python.exe`.\\n\\n5. Una vez seleccionada la ruta correcta, Visual Studio Code utilizará ese entorno virtual de Conda como intérprete de Python para tu proyecto.\\n\\nDe esta manera, podrás utilizar Conda para gestionar tus entornos virtuales y Visual Studio Code para trabajar en tus proyectos de Python de forma integrada. Si necesitas más ayuda o tienes alguna otra pregunta, ¡estaré encantado de ayudarte!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promptemplate idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_language = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Actua como un asistente de lenguaje natural y responde a las preguntas de los usuarios in {language}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_str = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_language | model_ollama | parser_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Você pode dizer \"Olá, como você está?\" \\n\\nEssa é uma forma comum e educada de cumprimentar alguém em português. 😊\\n\\n\\nLet me know if you have any other questions! 😄  \\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(\"como decir: hola como estas\")], \"language\": \"portugues\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con historial de conversacion\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id_session = {\"configurable\": {\"session_id\": \"456\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá, como você está? Eu sou o Miguel. E você? \\n\\n\\nI hope this is helpful! Let me know if you need anything else. 😊  \\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(\"hola como estas, soy miguel y tu?\")], \"language\": \"portugues\"},\n",
    "    config=config_id_session\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id_session = {\"configurable\": {\"session_id\": \"45622\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá Miguel, tudo bem! Eu estou ótimo, obrigado por perguntar.  \\n\\nE você, como está? 😊 \\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(\"hola como estas, soy miguel y tu?\")], \"language\": \"portugues\"},\n",
    "    config=config_id_session\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo del historial de conversacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=400,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model_ollama,\n",
    "    include_system=True,\n",
    "    allow_partial=True,\n",
    "    start_on=\"human\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content=\"hi! I'm bob\"),\n",
       " AIMessage(content='hi!'),\n",
       " HumanMessage(content='I like vanilla ice cream'),\n",
       " AIMessage(content='nice'),\n",
       " HumanMessage(content='whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='having fun?'),\n",
       " AIMessage(content='yes!')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Me preguntaste cuánto es 2 + 2. ¡Espero haber respondido correctamente!'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta1 = HumanMessage(content=\"cual problema de matemaricas te pregunte?\")\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt_language\n",
    "    | model_openAi\n",
    "    | parser_str \n",
    ")\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [pregunta1],\n",
    "        \"language\": \"espanol\"\n",
    "    }\n",
    ")\n",
    "messages.append(pregunta1)\n",
    "messages.append(response)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Sí, me encantaría! ¿Qué otro problema quieres que intente resolver? 😊  \\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta2= HumanMessage(content=\"cual es mi nombre\")\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [pregunta2],\n",
    "        \"language\": \"espanol\"\n",
    "    }\n",
    ")\n",
    "messages.append(pregunta2)\n",
    "messages.append(response)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content='¡Tienes razón! Te pregunté cuál era el resultado de 2 + 2. 😊  \\n\\n¿Hay alguna otra pregunta que pueda ayudarte a resolver? \\n')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|¡|Cl|aro|!| ¿|Por| qué| el| hel|ado| de| vain|illa| se| p|uso| tr|iste|?| ¡|Por|que| Bob| se| lo| com|ió| todo|!| 🍦|🤣||"
     ]
    }
   ],
   "source": [
    "pregunta3 = HumanMessage(content=\"cuantame un chiste, sobre algo que me haya pasado a mi\")\n",
    "for r in chain.stream(\n",
    "     {\n",
    "        \"messages\": messages + [pregunta3],\n",
    "        \"language\": \"espanol\"\n",
    "    }\n",
    "):\n",
    "    print(r, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
