{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, trim_messages, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import (BaseChatMessageHistory, InMemoryChatMessageHistory)\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_openAi = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "model_ollama = OllamaLLM(model=\"gemma2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have feelings or experiences like humans do. But thanks for asking!\\n\\nHow are you doing today? üòä\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ollama.invoke([HumanMessage(\"Hola como estas?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Como modelo de lenguaje, no tengo acceso a informaci√≥n personal como tu nombre.\\n\\nPara que pueda ayudarte con tu pregunta, por favor, dime tu nombre. üòä'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ollama.invoke([HumanMessage(\"cual es mi nombre?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Como modelo ling√º√≠stico de IA, no tengo hermanos ni familia. Soy un programa inform√°tico dise√±ado para ayudar a las personas con tareas como generar texto, traducir idiomas y responder preguntas.  \\n\\n¬øHay algo m√°s en lo que pueda ayudarte? üòä \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ollama.invoke(\n",
    "    [\n",
    "        HumanMessage(\"Cual es el nombre de la persona que me atiende?\"),\n",
    "        SystemMessage(\"Mi nombre es Gemma2\"),\n",
    "        HumanMessage(\"Gemma2, tienes algun hermano?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model_openAi, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creando un session id\n",
    "config_id = {\"configurable\": {\"session_id\": \"1234\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"que tama√±o de monitor es recomentable para trabajar?\")],\n",
    "    config=config_id\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El tama√±o recomendable de un monitor para trabajar depende de las necesidades y preferencias del usuario. Generalmente, se considera que un monitor de 24 pulgadas es adecuado para la mayor√≠a de las tareas de trabajo, ya que proporciona suficiente espacio en pantalla para multitareas y una buena visualizaci√≥n de contenido.\\n\\nSin embargo, si se requiere de m√°s espacio en pantalla, se puede optar por monitores de 27 pulgadas o incluso de 32 pulgadas, que ofrecen una mayor √°rea de visualizaci√≥n y son ideales para tareas que requieren trabajar con m√∫ltiples ventanas abiertas al mismo tiempo.\\n\\nPor otro lado, si se necesita un monitor m√°s port√°til y con menor espacio ocupado en el escritorio, se pueden considerar monitores de 21 o 22 pulgadas.\\n\\nEn resumen, el tama√±o recomendable de un monitor para trabajar depender√° de las necesidades espec√≠ficas de cada usuario, pero en general, se considera que un monitor de 24 pulgadas es una buena opci√≥n para la mayor√≠a de las personas.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Para un monitor de 32 pulgadas, se recomienda una resoluci√≥n de al menos 2560 x 1440 p√≠xeles (QHD) para garantizar una buena calidad de imagen y una visualizaci√≥n n√≠tida de los contenidos. Esto proporcionar√° una mayor densidad de p√≠xeles en comparaci√≥n con monitores de menor tama√±o, lo que resultar√° en una experiencia visual m√°s inmersiva.\\n\\nAdem√°s, es importante considerar la distancia a la que se va a utilizar el monitor. Para un monitor de 32 pulgadas, se recomienda colocarlo a una distancia de aproximadamente 80-100 cent√≠metros del usuario para una visualizaci√≥n c√≥moda y ergon√≥mica.\\n\\nEn cuanto a la configuraci√≥n de color, se sugiere calibrar el monitor para obtener una reproducci√≥n precisa de los colores. Se puede utilizar un dispositivo de calibraci√≥n de color o ajustar manualmente los ajustes de brillo, contraste y temperatura de color para lograr una imagen equilibrada y natural.\\n\\nEn resumen, para un monitor de 32 pulgadas, se recomienda una resoluci√≥n de al menos 2560 x 1440 p√≠xeles, una distancia de visualizaci√≥n adecuada y una configuraci√≥n de color precisa para una experiencia de trabajo √≥ptima.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"que configuracion es recomendable para un monitor de 32 pulgadas?\")],\n",
    "    config=config_id\n",
    "    \n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promptemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Actua como un asistente de desarrollo de software y responde a las preguntas de los usuarios\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model_openAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Para instalar Python en tu computadora, sigue estos pasos:\\n\\n1. Ve al sitio web oficial de Python en https://www.python.org/downloads/\\n2. Descarga la √∫ltima versi√≥n estable de Python para tu sistema operativo (Windows, macOS o Linux).\\n3. Ejecuta el instalador descargado y sigue las instrucciones en pantalla.\\n4. Durante la instalaci√≥n, aseg√∫rate de marcar la casilla que dice \"Add Python to PATH\" (Agregar Python al PATH) para poder usar Python desde la l√≠nea de comandos.\\n5. Una vez finalizada la instalaci√≥n, puedes verificar que Python se ha instalado correctamente abriendo una terminal (en Windows, puedes abrir el s√≠mbolo del sistema o PowerShell) y escribiendo `python --version` o `python3 --version`.\\n\\n¬°Listo! Ahora tienes Python instalado en tu computadora y puedes empezar a programar en este lenguaje.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(\"como puedo instalar python?\")]})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregando message history al prompt template\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id_session = {\"configurable\": {\"session_id\": \"789\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"usando conda puedo instalar diferentes versiones de python?\")],\n",
    "    config=config_id_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S√≠, con Conda puedes crear entornos virtuales con diferentes versiones de Python. Por ejemplo, puedes crear un entorno con Python 3.6 y otro con Python 3.8. De esta manera, puedes trabajar en proyectos que requieran versiones espec√≠ficas de Python sin que entren en conflicto entre s√≠. Aqu√≠ te dejo un ejemplo de c√≥mo crear un entorno con una versi√≥n espec√≠fica de Python utilizando Conda:\\n\\n1. Para crear un nuevo entorno con una versi√≥n espec√≠fica de Python, puedes utilizar el siguiente comando en la terminal:\\n\\n```\\nconda create -n mi_entorno python=3.6\\n```\\n\\nEste comando crear√° un nuevo entorno llamado \"mi_entorno\" con Python 3.6 instalado.\\n\\n2. Para activar el entorno reci√©n creado, puedes utilizar el siguiente comando:\\n\\n```\\nconda activate mi_entorno\\n```\\n\\nUna vez activado el entorno, puedes instalar las librer√≠as y paquetes necesarios para tu proyecto en ese entorno espec√≠fico.\\n\\nRecuerda que es importante mantener los entornos virtuales separados para evitar conflictos entre las diferentes versiones de Python y las dependencias de tus proyectos. ¬°Espero que esta informaci√≥n te haya sido √∫til! Si tienes alguna otra pregunta, no dudes en dec√≠rmelo.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(\"y puedo usala con visual studio code?\")],\n",
    "    config=config_id_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S√≠, puedes utilizar Conda con Visual Studio Code para gestionar entornos virtuales y trabajar en proyectos de Python de manera m√°s organizada. Aqu√≠ te explico c√≥mo puedes configurar Visual Studio Code para que reconozca y utilice entornos virtuales de Conda:\\n\\n1. Abre Visual Studio Code y aseg√∫rate de tener instalada la extensi√≥n \"Python\" de Microsoft.\\n\\n2. En la parte inferior izquierda de la ventana de Visual Studio Code, ver√°s un bot√≥n que indica el int√©rprete de Python actualmente seleccionado. Haz clic en este bot√≥n y selecciona \"Seleccionar int√©rprete\".\\n\\n3. En la lista de int√©rpretes que aparece, busca la opci√≥n \"Enter interpreter path...\" y haz clic en ella.\\n\\n4. En la ventana que se abre, introduce la ruta al int√©rprete de Python que se encuentra dentro del entorno virtual que creaste con Conda. Por lo general, la ruta tendr√° una estructura similar a esta: `C:\\\\Users\\\\tu_usuario\\\\Anaconda3\\\\envs\\\\mi_entorno\\\\python.exe`.\\n\\n5. Una vez seleccionada la ruta correcta, Visual Studio Code utilizar√° ese entorno virtual de Conda como int√©rprete de Python para tu proyecto.\\n\\nDe esta manera, podr√°s utilizar Conda para gestionar tus entornos virtuales y Visual Studio Code para trabajar en tus proyectos de Python de forma integrada. Si necesitas m√°s ayuda o tienes alguna otra pregunta, ¬°estar√© encantado de ayudarte!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promptemplate idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_language = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Actua como un asistente de lenguaje natural y responde a las preguntas de los usuarios in {language}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_str = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_language | model_ollama | parser_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Voc√™ pode dizer \"Ol√°, como voc√™ est√°?\" \\n\\nEssa √© uma forma comum e educada de cumprimentar algu√©m em portugu√™s. üòä\\n\\n\\nLet me know if you have any other questions! üòÑ  \\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(\"como decir: hola como estas\")], \"language\": \"portugues\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con historial de conversacion\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id_session = {\"configurable\": {\"session_id\": \"456\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ol√°, como voc√™ est√°? Eu sou o Miguel. E voc√™? \\n\\n\\nI hope this is helpful! Let me know if you need anything else. üòä  \\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(\"hola como estas, soy miguel y tu?\")], \"language\": \"portugues\"},\n",
    "    config=config_id_session\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id_session = {\"configurable\": {\"session_id\": \"45622\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ol√° Miguel, tudo bem! Eu estou √≥timo, obrigado por perguntar.  \\n\\nE voc√™, como est√°? üòä \\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(\"hola como estas, soy miguel y tu?\")], \"language\": \"portugues\"},\n",
    "    config=config_id_session\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo del historial de conversacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=400,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model_ollama,\n",
    "    include_system=True,\n",
    "    allow_partial=True,\n",
    "    start_on=\"human\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content=\"hi! I'm bob\"),\n",
       " AIMessage(content='hi!'),\n",
       " HumanMessage(content='I like vanilla ice cream'),\n",
       " AIMessage(content='nice'),\n",
       " HumanMessage(content='whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='having fun?'),\n",
       " AIMessage(content='yes!')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Me preguntaste cu√°nto es 2 + 2. ¬°Espero haber respondido correctamente!'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta1 = HumanMessage(content=\"cual problema de matemaricas te pregunte?\")\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt_language\n",
    "    | model_openAi\n",
    "    | parser_str \n",
    ")\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [pregunta1],\n",
    "        \"language\": \"espanol\"\n",
    "    }\n",
    ")\n",
    "messages.append(pregunta1)\n",
    "messages.append(response)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬°S√≠, me encantar√≠a! ¬øQu√© otro problema quieres que intente resolver? üòä  \\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta2= HumanMessage(content=\"cual es mi nombre\")\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [pregunta2],\n",
    "        \"language\": \"espanol\"\n",
    "    }\n",
    ")\n",
    "messages.append(pregunta2)\n",
    "messages.append(response)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content='¬°Tienes raz√≥n! Te pregunt√© cu√°l era el resultado de 2 + 2. üòä  \\n\\n¬øHay alguna otra pregunta que pueda ayudarte a resolver? \\n')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|¬°|Cl|aro|!| ¬ø|Por| qu√©| el| hel|ado| de| vain|illa| se| p|uso| tr|iste|?| ¬°|Por|que| Bob| se| lo| com|i√≥| todo|!| üç¶|ü§£||"
     ]
    }
   ],
   "source": [
    "pregunta3 = HumanMessage(content=\"cuantame un chiste, sobre algo que me haya pasado a mi\")\n",
    "for r in chain.stream(\n",
    "     {\n",
    "        \"messages\": messages + [pregunta3],\n",
    "        \"language\": \"espanol\"\n",
    "    }\n",
    "):\n",
    "    print(r, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
