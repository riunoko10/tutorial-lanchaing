{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"traduce los siguientes textos del ingles al español\"),\n",
    "    HumanMessage(content=\"\"\"While dense retrieval has been shown effective\n",
    "and efficient across tasks and languages,\n",
    "it remains difficult to create effective fully\n",
    "zero-shot dense retrieval systems when no relevance\n",
    "label is available. In this paper, we\n",
    "recognize the difficulty of zero-shot learning\n",
    "and encoding relevance. Instead, we propose\n",
    "to pivot through Hypothetical Document\n",
    "Embeddings (HyDE). Given a query, HyDE first\n",
    "zero-shot instructs an instruction-following\n",
    "language model (e.g. InstructGPT) to generate\n",
    "a hypothetical document. The document\n",
    "captures relevance patterns but is unreal\n",
    "and may contain false details. Then, an unsupervised\n",
    "contrastively learned encoder (e.g.\n",
    "Contriever) encodes the document into an\n",
    "embedding vector. This vector identifies a\n",
    "neighborhood in the corpus embedding space,\n",
    "where similar real documents are retrieved\n",
    "based on vector similarity. This second step\n",
    "ground the generated document to the actual\n",
    "corpus, with the encoder’s dense bottleneck\n",
    "filtering out the incorrect details. Our experiments\n",
    "show that HyDE significantly outperforms\n",
    "the state-of-the-art unsupervised dense\n",
    "retriever Contriever and shows strong performance\n",
    "comparable to fine-tuned retrievers,\n",
    "across various tasks (e.g. web search, QA, fact\n",
    "verification) and languages (e.g. sw, ko, ja).1\"\"\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traducción:\\n\\n\"Hola, ¿cómo estás?\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola, ¿cómo estás?!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aquí te presento la traducción al español del texto:\\n\\nMientras que el aprendizaje denso ha demostrado ser efectivo y eficiente en diferentes tareas y lenguas,\\npermanece difícil crear sistemas de retrieval denso efectivos cuando no se dispone de una etiqueta de relevancia. En este papel, reconocemos la dificultad del aprendizaje cero-shot y el encoding de relevancia. En su lugar, proponemos pivotar a través de Embeddings de Documentos Hipotéticos (HyDE). Dado un consulta, HyDE instruye inicialmente un modelo de lenguaje que sigue instrucciones (por ejemplo, InstructGPT) para generar un documento hipotético. El documento captura patrones de relevancia, pero es irreal y puede contener detalles falsos. Luego, un encoder aprendido de manera no supervisada y contrastiva (por ejemplo, Contriever) codifica el documento en una vector de embeddings. Este vector identifica un vecindario en el espacio de embeddings del corpus, donde se recuperan documentos reales similares basados en la similitud de vectores. Este segundo paso \"enraíza\" el documento generado con el corpus real, mientras que la boca densa del encoder filtra fuera los detalles incorrectos. Nuestros experimentos muestran que HyDE supera significativamente al estado del arte no supervisado Contriever y muestra un rendimiento fuerte comparable a los retrievers ajustados, en diferentes tareas (por ejemplo, búsqueda web, QA, verificación de hechos) y lenguas (por ejemplo, sv, ko, ja).'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
