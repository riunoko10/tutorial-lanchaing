{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection('blog', embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(client=chroma_client, collection_name='blog', embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\":0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_message = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(example_message[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programacion\\langchain\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'id': 46, 'metadata': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'page_content': 'Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",', 'tokens': 'here are sample conversation for task clarification sent to openai chatcompletion endpoint used by gptengineer the user inputs are wrapped in user input text role system content you will read instructions and not carry them out only seek to clarify themnspecifically you will first summarise list of super short bullets of areas that need clarificationnthen you will pick one clarifying question and wait for an answer from the usern role user content we are writing super mario game in python mvc components split in separate files keyboard controln role assistant'}, page_content='here are sample conversation for task clarification sent to openai chatcompletion endpoint used by gptengineer the user inputs are wrapped in user input text role system content you will read instructions and not carry them out only seek to clarify themnspecifically you will first summarise list of super short bullets of areas that need clarificationnthen you will pick one clarifying question and wait for an answer from the usern role user content we are writing super mario game in python mvc components split in separate files keyboard controln role assistant'), -0.34635622121077403), (Document(metadata={'id': 6, 'metadata': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'page_content': 'Fig. 3. (Left) The agreement rate is measured by comparing each pair of translation sentences (\"A > B\", \"A=B\", \"A < B\") and thus chance agreement is 1/3. The upper bound is set by the expert-expert agreement rate. (Right) Comparison of BLEU score between translations from different sources. LCD (Linguistic Data Consortium) translators provide expert translations. (Image source: Callison-Burch 2009)\\nRater Agreement#\\nWe often think of annotation as targeting a single ground truth and try to evaluate quality against one gold answer with consistent standards. A common practice for finding reliable ground truth labels is to collect multiple labels from multiple raters. Assuming that each rater performs at a different level of quality, we can use a weighted average of annotations but weighted by a proficiency score. This score is often approximated by how often one rater agrees with others.', 'tokens': 'fig left the agreement rate is measured by comparing each pair of translation sentences b b b and thus chance agreement is the upper bound is set by the expert expert agreement rate right comparison of bleu score between translations from different sources lcd linguistic data consortium translators provide expert translations image source callison burch rater agreement we often think of annotation as targeting single ground truth and try to evaluate quality against one gold answer with consistent standards common practice for finding reliable ground truth labels is to collect multiple labels from multiple raters assuming that each rater performs at different level of quality we can use weighted average of annotations but weighted by proficiency score this score is often approximated by how often one rater agrees with others'}, page_content='fig left the agreement rate is measured by comparing each pair of translation sentences b b b and thus chance agreement is the upper bound is set by the expert expert agreement rate right comparison of bleu score between translations from different sources lcd linguistic data consortium translators provide expert translations image source callison burch rater agreement we often think of annotation as targeting single ground truth and try to evaluate quality against one gold answer with consistent standards common practice for finding reliable ground truth labels is to collect multiple labels from multiple raters assuming that each rater performs at different level of quality we can use weighted average of annotations but weighted by proficiency score this score is often approximated by how often one rater agrees with others'), -0.35604348597945346), (Document(metadata={'id': 13, 'metadata': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'page_content': 'Often there is more than one correct interpretation for some samples. We need diverse perspectives via e.g. having multiple people to review annotation quality.\\nDisagreement is not always bad. We should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information.\\n\\nIf it is caused by a task not well defined, we should enhance the instruction. However, a more detailed guideline does not resolve innate diversity among opinions.\\n\\n\\nExperts may not always be better than lay people, but they would have a big gap in terms of considering what’s important.\\nGround truth annotations can change in time, especially those related to timely events or news.\\n\\nLater, Rottger et al. (2021) formulated the difference into two contrasting paradigms for data annotation for subjective NLP tasks.\\n\\n\\n\\n\\nDescriptive\\nPrescriptive', 'tokens': 'often there is more than one correct interpretation for some samples we need diverse perspectives via g having multiple people to review annotation quality disagreement is not always bad we should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information if it is caused by task not well defined we should enhance the instruction however more detailed guideline does not resolve innate diversity among opinions experts may not always be better than lay people but they would have big gap in terms of considering what s important ground truth annotations can change in time especially those related to timely events or news later rottger et formulated the difference into two contrasting paradigms for data annotation for subjective nlp tasks descriptive prescriptive'}, page_content='often there is more than one correct interpretation for some samples we need diverse perspectives via g having multiple people to review annotation quality disagreement is not always bad we should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information if it is caused by task not well defined we should enhance the instruction however more detailed guideline does not resolve innate diversity among opinions experts may not always be better than lay people but they would have big gap in terms of considering what s important ground truth annotations can change in time especially those related to timely events or news later rottger et formulated the difference into two contrasting paradigms for data annotation for subjective nlp tasks descriptive prescriptive'), -0.36233750613468674), (Document(metadata={'id': 18, 'metadata': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'page_content': 'Fig. 5. A taxonomy of causes for rater disagreement. (Image source: Zhang et al. 2023)\\nDisagreement deconvolution relies on probabilistic graph modeling:\\n\\nEstimate how often an annotator returns non-primary labels, $p_\\\\text{flip}$\\nPer sample, get an adjusted label distribution $p^*$ of primary labels based on $p_\\\\text{flip}$\\nSample from $p^*$ as a new test set.\\nMeasure performance metrics against the new test set.\\n\\nGiven $C$-category classification, the sampling process of the generative model is stated as follows:\\n\\n$$\\n\\\\begin{aligned}\\ny^*\\\\mid x &\\\\sim \\\\text{Categorial}([C], p^*(y\\\\mid x)) \\\\\\\\\\ny_\\\\text{other}\\\\mid y^* &\\\\sim \\\\text{Categorial}([C]\\\\setminus\\\\{y^*\\\\}, \\\\frac{1}{C-1}) \\\\\\\\\\nz_\\\\text{flip} \\\\mid x &\\\\sim \\\\text{Bernoulli}(p_\\\\text{flip}(x)) \\\\\\\\\\ny\\\\mid y^*, y_\\\\text{other}, z_\\\\text{flip} &= y^* (1 - z_\\\\text{flip}) + y_\\\\text{other} z_\\\\text{flip}\\n\\\\end{aligned}\\n$$', 'tokens': 'fig taxonomy of causes for rater disagreement image source zhang et disagreement deconvolution relies on probabilistic graph modeling estimate how often an annotator returns non primary labels p text flip per sample get an adjusted label distribution p of primary labels based on p text flip sample from p as new test set measure performance metrics against the new test set given c category classification the sampling process of the generative model is stated as follows begin aligned mid x sim text categorial c p mid x text other mid sim text categorial c setminus frac c z text flip mid x sim text bernoulli p text flip x mid text other z text flip z text flip text other z text flip end aligned'}, page_content='fig taxonomy of causes for rater disagreement image source zhang et disagreement deconvolution relies on probabilistic graph modeling estimate how often an annotator returns non primary labels p text flip per sample get an adjusted label distribution p of primary labels based on p text flip sample from p as new test set measure performance metrics against the new test set given c category classification the sampling process of the generative model is stated as follows begin aligned mid x sim text categorial c p mid x text other mid sim text categorial c setminus frac c z text flip mid x sim text bernoulli p text flip x mid text other z text flip z text flip text other z text flip end aligned'), -0.36454765602303096)]\n",
      "  warnings.warn(\n",
      "d:\\Programacion\\langchain\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.5\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rater agreement is a statistical measure that assesses the consistency of judgments made by two or more raters. It quantifies the level of agreement between raters on a particular task, such as coding text or evaluating performance.  Higher rater agreement indicates greater consistency and reliability in the ratings. \n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "for chunk in rag_chain.stream(\"What is Rater Agreement?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear una Chain personalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    'Usted es un asistente para tareas de respuesta a preguntas. '\n",
    "    'Si el texto de entrada esta en un idioma diferente al español, traduzca el texto al español.'\n",
    "    'Utilice las siguientes piezas de contexto recuperado para responder '\n",
    "    'la pregunta. Si no sabes la respuesta, di que '\n",
    "    'No lo sé. Utilice tres oraciones como máximo y mantenga el '\n",
    "    'respuesta concisa.'\n",
    "    '\\n\\n'\n",
    "    '{context}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programacion\\langchain\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'id': 18, 'metadata': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'page_content': 'Fig. 5. A taxonomy of causes for rater disagreement. (Image source: Zhang et al. 2023)\\nDisagreement deconvolution relies on probabilistic graph modeling:\\n\\nEstimate how often an annotator returns non-primary labels, $p_\\\\text{flip}$\\nPer sample, get an adjusted label distribution $p^*$ of primary labels based on $p_\\\\text{flip}$\\nSample from $p^*$ as a new test set.\\nMeasure performance metrics against the new test set.\\n\\nGiven $C$-category classification, the sampling process of the generative model is stated as follows:\\n\\n$$\\n\\\\begin{aligned}\\ny^*\\\\mid x &\\\\sim \\\\text{Categorial}([C], p^*(y\\\\mid x)) \\\\\\\\\\ny_\\\\text{other}\\\\mid y^* &\\\\sim \\\\text{Categorial}([C]\\\\setminus\\\\{y^*\\\\}, \\\\frac{1}{C-1}) \\\\\\\\\\nz_\\\\text{flip} \\\\mid x &\\\\sim \\\\text{Bernoulli}(p_\\\\text{flip}(x)) \\\\\\\\\\ny\\\\mid y^*, y_\\\\text{other}, z_\\\\text{flip} &= y^* (1 - z_\\\\text{flip}) + y_\\\\text{other} z_\\\\text{flip}\\n\\\\end{aligned}\\n$$', 'tokens': 'fig taxonomy of causes for rater disagreement image source zhang et disagreement deconvolution relies on probabilistic graph modeling estimate how often an annotator returns non primary labels p text flip per sample get an adjusted label distribution p of primary labels based on p text flip sample from p as new test set measure performance metrics against the new test set given c category classification the sampling process of the generative model is stated as follows begin aligned mid x sim text categorial c p mid x text other mid sim text categorial c setminus frac c z text flip mid x sim text bernoulli p text flip x mid text other z text flip z text flip text other z text flip end aligned'}, page_content='fig taxonomy of causes for rater disagreement image source zhang et disagreement deconvolution relies on probabilistic graph modeling estimate how often an annotator returns non primary labels p text flip per sample get an adjusted label distribution p of primary labels based on p text flip sample from p as new test set measure performance metrics against the new test set given c category classification the sampling process of the generative model is stated as follows begin aligned mid x sim text categorial c p mid x text other mid sim text categorial c setminus frac c z text flip mid x sim text bernoulli p text flip x mid text other z text flip z text flip text other z text flip end aligned'), -0.3746557564841462), (Document(metadata={'id': 42, 'metadata': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'page_content': 'Commands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"', 'tokens': 'commands google search google args input search browse website browsewebsite args url url question whatyouwanttofindonwebsite start gpt agent startagent args name name task shorttaskdesc prompt prompt message gpt agent messageagent args key key message message list gpt agents listagents args delete gpt agent deleteagent args key key clone repository clonerepository args repositoryurl url clonepath directory write to file writetofile args file file text text read file readfile args file file append to file appendtofile args file file text text delete file deletefile args file file search files searchfiles args directory directory analyze code analyzecode args code fullcodestring'}, page_content='commands google search google args input search browse website browsewebsite args url url question whatyouwanttofindonwebsite start gpt agent startagent args name name task shorttaskdesc prompt prompt message gpt agent messageagent args key key message message list gpt agents listagents args delete gpt agent deleteagent args key key clone repository clonerepository args repositoryurl url clonepath directory write to file writetofile args file file text text read file readfile args file file append to file appendtofile args file file text text delete file deletefile args file file search files searchfiles args directory directory analyze code analyzecode args code fullcodestring'), -0.39092934754147324), (Document(metadata={'id': 6, 'metadata': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'page_content': 'Fig. 3. (Left) The agreement rate is measured by comparing each pair of translation sentences (\"A > B\", \"A=B\", \"A < B\") and thus chance agreement is 1/3. The upper bound is set by the expert-expert agreement rate. (Right) Comparison of BLEU score between translations from different sources. LCD (Linguistic Data Consortium) translators provide expert translations. (Image source: Callison-Burch 2009)\\nRater Agreement#\\nWe often think of annotation as targeting a single ground truth and try to evaluate quality against one gold answer with consistent standards. A common practice for finding reliable ground truth labels is to collect multiple labels from multiple raters. Assuming that each rater performs at a different level of quality, we can use a weighted average of annotations but weighted by a proficiency score. This score is often approximated by how often one rater agrees with others.', 'tokens': 'fig left the agreement rate is measured by comparing each pair of translation sentences b b b and thus chance agreement is the upper bound is set by the expert expert agreement rate right comparison of bleu score between translations from different sources lcd linguistic data consortium translators provide expert translations image source callison burch rater agreement we often think of annotation as targeting single ground truth and try to evaluate quality against one gold answer with consistent standards common practice for finding reliable ground truth labels is to collect multiple labels from multiple raters assuming that each rater performs at different level of quality we can use weighted average of annotations but weighted by proficiency score this score is often approximated by how often one rater agrees with others'}, page_content='fig left the agreement rate is measured by comparing each pair of translation sentences b b b and thus chance agreement is the upper bound is set by the expert expert agreement rate right comparison of bleu score between translations from different sources lcd linguistic data consortium translators provide expert translations image source callison burch rater agreement we often think of annotation as targeting single ground truth and try to evaluate quality against one gold answer with consistent standards common practice for finding reliable ground truth labels is to collect multiple labels from multiple raters assuming that each rater performs at different level of quality we can use weighted average of annotations but weighted by proficiency score this score is often approximated by how often one rater agrees with others'), -0.40165846100304403), (Document(metadata={'id': 13, 'metadata': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'page_content': 'Often there is more than one correct interpretation for some samples. We need diverse perspectives via e.g. having multiple people to review annotation quality.\\nDisagreement is not always bad. We should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information.\\n\\nIf it is caused by a task not well defined, we should enhance the instruction. However, a more detailed guideline does not resolve innate diversity among opinions.\\n\\n\\nExperts may not always be better than lay people, but they would have a big gap in terms of considering what’s important.\\nGround truth annotations can change in time, especially those related to timely events or news.\\n\\nLater, Rottger et al. (2021) formulated the difference into two contrasting paradigms for data annotation for subjective NLP tasks.\\n\\n\\n\\n\\nDescriptive\\nPrescriptive', 'tokens': 'often there is more than one correct interpretation for some samples we need diverse perspectives via g having multiple people to review annotation quality disagreement is not always bad we should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information if it is caused by task not well defined we should enhance the instruction however more detailed guideline does not resolve innate diversity among opinions experts may not always be better than lay people but they would have big gap in terms of considering what s important ground truth annotations can change in time especially those related to timely events or news later rottger et formulated the difference into two contrasting paradigms for data annotation for subjective nlp tasks descriptive prescriptive'}, page_content='often there is more than one correct interpretation for some samples we need diverse perspectives via g having multiple people to review annotation quality disagreement is not always bad we should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information if it is caused by task not well defined we should enhance the instruction however more detailed guideline does not resolve innate diversity among opinions experts may not always be better than lay people but they would have big gap in terms of considering what s important ground truth annotations can change in time especially those related to timely events or news later rottger et formulated the difference into two contrasting paradigms for data annotation for subjective nlp tasks descriptive prescriptive'), -0.40194134049740105)]\n",
      "  warnings.warn(\n",
      "d:\\Programacion\\langchain\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.5\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a large, complex task into smaller, more manageable subtasks. This makes the task easier to understand, plan, and execute. \n",
      "\n",
      "For example, writing a book can be decomposed into tasks like outlining, researching, drafting, editing, and proofreading. Building a house can be decomposed into tasks like laying the foundation, framing the walls, installing plumbing, and painting the interior. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain_2 = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain_2.invoke({\"input\": \"What is Task Decomposition? get me the examples of task decomposition\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
